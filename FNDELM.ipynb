{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9adb9-311d-40a2-a137-291408c68b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import random\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17930932-afec-4c23-bb40-e458a976fecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d80d3-c105-4d95-8dae-f55cff6254ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def train_ensemble(X, y, classifiers, max_step, rho, batch_size):\n",
    "\n",
    "    n = len(X)\n",
    "    K = len(classifiers)\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    initial_lr = 0.1\n",
    "    decay_rate = 0.01 \n",
    "\n",
    "    predictions_matrix = ensemble_predict(X, classifiers)\n",
    "    diversity_matrix = precompute_diversity_parallel(predictions_matrix, y)\n",
    "    w = initialize_weights(classifiers, predictions_matrix, 0.6, y) \n",
    "    current_z = 0\n",
    "\n",
    "    for t in range(max_step):\n",
    "        indices = np.random.permutation(n)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        predictions_matrix_shuffled = predictions_matrix[indices]\n",
    "\n",
    "        learning_rate = initial_lr / (1 + decay_rate * t)\n",
    "        for j in range(0, n, batch_size):\n",
    "            X_batch = X[j:j + batch_size]\n",
    "            y_batch = y[j:j + batch_size]\n",
    "            predictions_matrix_batch = predictions_matrix_shuffled[j:j + batch_size] \n",
    "            F_X_batch = np.dot(predictions_matrix_batch, w) \n",
    "\n",
    "            gradient = compute_gradient(X_batch, y_batch, F_X_batch, predictions_matrix_batch, classifiers, rho, diversity_matrix)\n",
    "            w = w - learning_rate * gradient\n",
    "            w = w_standard(w)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "        F_X = np.dot(predictions_matrix, w)\n",
    "        last_z = objective_function(w, X, y, F_X, predictions_matrix, classifiers, rho, diversity_matrix)\n",
    "\n",
    "        if t > 0 and abs(last_z - current_z) < 1e-5:\n",
    "            break\n",
    "        \n",
    "        current_z = last_z\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb1701-1a3a-4580-a998-cce638c61ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier(X, y, subset_num):\n",
    "    n_samples = len(X)\n",
    "    classifiers = Parallel(n_jobs=1)(delayed(train_single_classifier)(X, y, n_samples, i) for i in range(subset_num))\n",
    "    return classifiers\n",
    "\n",
    "def train_single_classifier(X, y, n_samples, i):\n",
    "    X_subset, y_subset = resample(X, y, n_samples=n_samples, random_state=i*10, replace=True)\n",
    "    classifier = XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=i*10, eval_metric = 'logloss')\n",
    "    classifier.fit(X_subset, y_subset, verbose=True)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446f887-9f7a-4167-89ac-c995adc04055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(classifiers):\n",
    "    params_list = []\n",
    "\n",
    "    for model in classifiers:\n",
    "        df = model.get_booster().trees_to_dataframe()\n",
    "        total_params = (df['Feature'] == 'Leaf').sum()\n",
    "        params_list.append(total_params)\n",
    "\n",
    "    return params_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f83ed-744a-4f28-a51b-a6490d36d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_AIC(params, y_true, y_pred):\n",
    "    return 2 * params - 2 * np.sum(cross_loss(y_true, y_pred))\n",
    "\n",
    "def compute_BIC(params, m, y_true, y_pred):\n",
    "    return np.log(m) * params - 2 * np.sum(cross_loss(y_true, y_pred))\n",
    "\n",
    "def initialize_weights(classifiers, predictions_matrix, lambda_A, y):\n",
    "    AIC_list = []\n",
    "    BIC_list = []\n",
    "    y_np = y\n",
    "    \n",
    "    params_list = get_param(classifiers)\n",
    "    \n",
    "    for k in range(predictions_matrix.shape[1]):\n",
    "        pred_prob = predictions_matrix[:, k]\n",
    "        params_k = params_list[k]\n",
    "        AIC_k = compute_AIC(params_k, y_np, pred_prob)\n",
    "        BIC_k = compute_BIC(params_k, len(y), y_np, pred_prob)\n",
    "        AIC_list.append(AIC_k)\n",
    "        BIC_list.append(BIC_k)\n",
    "\n",
    "    w_tilde = lambda_A * np.array(AIC_list) + (1 - lambda_A) * np.array(BIC_list)\n",
    "    w_normalized = 1 / w_tilde\n",
    "    return w_normalized / np.sum(w_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a5d26-7902-4ca8-85f9-6612a63e80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensemble_predict(X, classifiers):\n",
    "    K = len(classifiers)\n",
    "    \n",
    "    predictions_matrix = np.zeros((len(X), K))  # (n_samlpe, K)\n",
    "    for k in range(K):\n",
    "        predictions_matrix[:, k] = classifiers[k].predict_proba(X)[:, 1] \n",
    "\n",
    "    return predictions_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84155b-f475-4ea2-b016-6c1c3dd5fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_div_entry(correct_matrix, k, l):\n",
    "    ck = correct_matrix[:, k]\n",
    "    cl = correct_matrix[:, l]\n",
    "    N11 = np.sum((ck == 1) & (cl == 1))\n",
    "    N10 = np.sum((ck == 1) & (cl == 0))\n",
    "    N01 = np.sum((ck == 0) & (cl == 1))\n",
    "    N00 = np.sum((ck == 0) & (cl == 0))\n",
    "    div = N00 / (N00 + N01 + N10 + N11)\n",
    "    return div\n",
    "\n",
    "def precompute_diversity_parallel(predictions_matrix, y_true):\n",
    "    K = predictions_matrix.shape[1]\n",
    "    binary_preds = (predictions_matrix >= 0.5).astype(int)\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    correct_matrix = (binary_preds == y_true).astype(int)\n",
    "\n",
    "    diversity_matrix = np.zeros((K, K))\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            div = compute_div_entry(correct_matrix, k, l)\n",
    "            diversity_matrix[k, l] = div\n",
    "            diversity_matrix[l, k] = div\n",
    "\n",
    "    return diversity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b963ad-7b7a-454d-9a1d-b08c5249bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_loss(y_true, y_pred):\n",
    "    epsilon = 1e-6\n",
    "    return ((y_true * np.log(y_pred + epsilon)) + ((1 - y_true) * np.log(1 - y_pred + epsilon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35483748-f224-4e84-9f32-564f587a4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective_function(w, X, y, F_X, predictions_matrix, classifiers, rho, diversity_matrix):\n",
    "    loss = -(np.sum(cross_loss(y, F_X))) / len(X)\n",
    "    diversity = np.dot(np.dot(diversity_matrix, np.ones(len(w))), w)\n",
    "    return loss + rho * diversity\n",
    "\n",
    "def compute_gradient(X, y, F_X, predictions_matrix, classifiers, rho, diversity_matrix):\n",
    "    K = len(classifiers)\n",
    "    grad = np.zeros(K)\n",
    "    y_np = y\n",
    "\n",
    "    grad_matrix = predictions_matrix / ((1 - y_np).reshape(-1, 1) - F_X.reshape(-1, 1))\n",
    "    grad = np.sum(grad_matrix, axis=0) / len(X)  \n",
    "    \n",
    "    grad += rho * np.dot(diversity_matrix, np.ones(K)) \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17838083-3ef8-4c41-9eb9-81fe6744d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_standard(w):\n",
    "    min_val = np.min(w)\n",
    "    max_val = np.max(w)\n",
    "    \n",
    "    if max_val <= 0:\n",
    "        w += abs(min_val)\n",
    "        min_val = 0\n",
    "        max_val = np.max(w)\n",
    "    \n",
    "    w_normalized = (w - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return w_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ea9b1-7b42-4fc2-b4ae-7999ef4f059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_part(X_train, y_train, max_step, subset_num, rho, batch_size):\n",
    "    classifiers = train_classifier(X_train, y_train, subset_num)\n",
    "    w_optimal = train_ensemble(X_train, y_train, classifiers, max_step, rho, batch_size) ##\n",
    "    return classifiers, w_optimal\n",
    "    \n",
    "\n",
    "def classification(X_test, y_test, classifiers, w_optimal):\n",
    "    predictions_matrix = ensemble_predict(X_test, classifiers)\n",
    "    F_X = np.dot(predictions_matrix, w_optimal)\n",
    "    y_pred = (F_X >= 0.5).astype(int)\n",
    "    probabilities = F_X\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "    TN, FP, _, _ = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    \n",
    "    g_mean = (recall * specificity) ** 0.5\n",
    "\n",
    "    results = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'G-Mean': g_mean,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': roc_auc\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e4996-afad-486a-8b35-8acf653a9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset(data):\n",
    "    data_X = data.drop(columns=['Label'])\n",
    "    data_y = data['Label']\n",
    "    \n",
    "    return data_X, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd0af2-8d94-4e4f-a2dd-6f951b71d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cross_validate_model(X, y, max_step, subset_num, rho, batch_size, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    all_metrics = []\n",
    "    all_times = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Fold {fold}...\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        start_time = time.time()\n",
    "        classifiers, w_optimal = train_part(X_train, y_train, max_step, subset_num, rho, batch_size)\n",
    "        results = classification(X_test, y_test, classifiers, w_optimal)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        results['Time (s)'] = elapsed_time\n",
    "\n",
    "        print(f\"Fold {fold} completed in {elapsed_time:.2f} seconds.\")\n",
    "        all_metrics.append(results)\n",
    "        all_times.append(elapsed_time)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    results_df = pd.DataFrame(all_metrics)\n",
    "    mean_results = results_df.mean().to_dict()\n",
    "    std_results = results_df.std().to_dict()\n",
    "\n",
    "    print(\"\\n=== Average Performance (10-Fold) ===\")\n",
    "    for key in mean_results:\n",
    "        print(f\"{key}: {mean_results[key]:.4f} ± {std_results[key]:.4f}\")\n",
    "\n",
    "    return classifiers, w_optimal, results_df, mean_results, std_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54837f-d4a5-4556-bcf6-69b047ebb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "def preprocess_data(df, scaling_method='minmax'):\n",
    "    df = df.copy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['Label'] = label_encoder.fit_transform(df['Label'].values)\n",
    "\n",
    "    labels = df['Label']\n",
    "    features = df.drop(columns=['Label'])\n",
    "\n",
    "    scaler = MinMaxScaler() if scaling_method == 'minmax' else StandardScaler()\n",
    "    features_df = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "    process_data = pd.concat([features_df, labels.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "    return process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19628d26-b0fe-47ef-b8e3-44acbdf95cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52694a88-e435-4e48-989f-0085d610957a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "data = pd.read_csv(r'')\n",
    "data = preprocess_data(data, scaling_method='minmax')\n",
    "\n",
    "data_X, data_y = dataset(data)\n",
    "classifiers, w_optimal, results, mean_results, std_results = cross_validate_model(\n",
    "    data_X,\n",
    "    data_y,\n",
    "    max_step=50,\n",
    "    subset_num=30,\n",
    "    rho=0.1,\n",
    "    batch_size=16,\n",
    "    n_splits=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4805d3-1fd2-4b73-aca8-84e1c21b6618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
