{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ebbec-4382-4fc9-88e6-b197cc2dac20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import re\n",
    "import textstat\n",
    "import liwc\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23778434-89a4-49b1-bc25-63f5a130da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190967c7-cdbf-4ecc-9aae-4b032100c558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15e531-067a-451d-aa84-38fd674846fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47080c4b-9ea0-456e-8565-4d36b0c674aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac525aef-cb66-48ab-89b8-ea71ed618599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# textual_feature\n",
    "def textual_features(text):\n",
    "    text_length = len(text)\n",
    "    character_count = sum(c.isalnum() for c in text)\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = len(sent_tokenize(text))\n",
    "    if sentence_count > 0:\n",
    "        words_per_sentence = word_count / sentence_count\n",
    "    else:\n",
    "        words_per_sentence = 0\n",
    "\n",
    "    special_symbols = len(re.findall(r'[?!#@]', text))\n",
    "    uppercase_count = sum(1 for c in text if c.isupper())\n",
    "\n",
    "    sentiment = TextBlob(text)\n",
    "    sentiment_score = sentiment.sentiment.polarity\n",
    "    subjectivity_score = sentiment.sentiment.subjectivity\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "    cli = textstat.coleman_liau_index(text)\n",
    "    fkg = textstat.flesch_kincaid_grade(text)\n",
    "    fre = textstat.flesch_reading_ease(text)\n",
    "    gfi = textstat.gunning_fog(text)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"text_length\": text_length, \n",
    "        \"character_count\": character_count,\n",
    "        \"word_count\": word_count, \n",
    "        \"sentence_count\": sentence_count, \n",
    "        \"words_per_sentence\": words_per_sentence,\n",
    "        \"special_symbols\": special_symbols,\n",
    "        \"uppercase_count\": uppercase_count,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"subjectivity_score\": subjectivity_score,\n",
    "        \"automated_readability_index\": ari,\n",
    "        \"coleman_liau_index\": cli, \n",
    "        \"flesch_kincaid_grade\": fkg, \n",
    "        \"flesch_reading_ease\": fre,\n",
    "        \"gunning_fog_index\": gfi\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4f441-a524-4d04-948b-7ce018a29576",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26952e4-cf2f-4a27-ba4e-33796e55416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import liwc\n",
    "\n",
    "##  lexicon_feature\n",
    "nltk.download('punkt')\n",
    "\n",
    "def count_liwc_categories(text, parse, target_categories):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    counts = defaultdict(int) \n",
    "\n",
    "    for token in tokens:\n",
    "        categories = parse(token)\n",
    "        for category in categories:\n",
    "            for target_category, liwc_categories in target_categories.items():\n",
    "                if category in liwc_categories:\n",
    "                    counts[target_category] += 1\n",
    "\n",
    "    all_categories = set(target_categories.keys())\n",
    "    final_counts = {category: counts.get(category, 0) for category in all_categories}\n",
    "\n",
    "    return final_counts\n",
    "\n",
    "def lexicon_feature(text):\n",
    "    liwc_path = 'LIWC2015 Dictionary.dic'\n",
    "    parse, category_names = liwc.load_token_parser(liwc_path)\n",
    "    \n",
    "    target_categories = {\n",
    "        'noun': ['pronoun'],\n",
    "        'verb': ['verb'],\n",
    "        'adjective': ['adj'],\n",
    "        'adverb': ['adverb'],\n",
    "        'auxverb': ['auxverb'],\n",
    "        'positive': ['posemo'],\n",
    "        'negative': ['negemo'],\n",
    "        'anxiety': ['anx'],\n",
    "        'anger': ['anger'],\n",
    "        'sad': ['sad']\n",
    "    }\n",
    "    \n",
    "    liwc_counts = count_liwc_categories(text, parse, target_categories)\n",
    "    liwc_series = pd.Series()\n",
    "    \n",
    "    for category in target_categories.keys():\n",
    "        count = liwc_counts.get(category, 0)\n",
    "        liwc_series[category] = count\n",
    "\n",
    "    return liwc_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dc7a3-f063-44d1-920f-bc07a3612fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## narrative-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40a614-100a-49d1-a622-3427e000a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_embedding(text, parse): \n",
    "    text_token = word_tokenize(text.lower())\n",
    "    vocabulary_count = Counter(category for token in text_token for category in parse(token))\n",
    "\n",
    "    if 'time' in vocabulary_count and 'cause' in vocabulary_count:\n",
    "        return 2\n",
    "    elif 'time' in vocabulary_count or 'cause' in vocabulary_count:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bbba8-5ec4-4399-b526-c2de8b569d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spacial_embedding(text, parse):\n",
    "    text_token = word_tokenize(text.lower())\n",
    "    vocabulary_count = Counter(category for token in text_token for category in parse(token))\n",
    "\n",
    "    if 'space' in vocabulary_count and 'percept' in vocabulary_count:\n",
    "        return 2\n",
    "    elif 'space' in vocabulary_count and 'percept' not in vocabulary_count:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275516fd-0b99-471d-b760-d7f601cadbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import liwc\n",
    "import os\n",
    "\n",
    "def calculate_features(text, parse, motion_categories, affective_categories, cognitive_categories):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    motion_count = []\n",
    "    affective_process_count = []\n",
    "    cognitive_insight_count = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        categories = [category for token in tokens for category in parse(token)]\n",
    "        motion_count.append(sum(1 for category in categories if category in motion_categories))\n",
    "        affective_process_count.append(sum(1 for category in categories if category in affective_categories))\n",
    "        cognitive_insight_count.append(sum(1 for category in categories if category in cognitive_categories))\n",
    "\n",
    "    affective_trigram_count = 0\n",
    "    cognitive_trigram_count = 0\n",
    "\n",
    "    for i in range(len(sentences) - 2):\n",
    "        if motion_count[i] > 0 and affective_process_count[i + 1] > 0 and motion_count[i + 2] > 0:\n",
    "            affective_trigram_count += 1\n",
    "        if motion_count[i] > 0 and cognitive_insight_count[i + 1] > 0 and motion_count[i + 2] > 0:\n",
    "            cognitive_trigram_count += 1\n",
    "\n",
    "    loa = affective_trigram_count / total_sentences if total_sentences > 0 else 0\n",
    "    loc = cognitive_trigram_count / total_sentences if total_sentences > 0 else 0\n",
    "\n",
    "    return loa, loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c1388-15c0-4a65-9694-3cda0f6fe898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ratio_senti(text):\n",
    "    text_sentence = sent_tokenize(text.lower())\n",
    "\n",
    "    sentence_data = []\n",
    "    for index, sentence in enumerate(text_sentence, start=1):\n",
    "        sentiment = TextBlob(sentence)\n",
    "        sentiment_score = sentiment.sentiment.polarity\n",
    "        ratio = index / len(text_sentence)\n",
    "        sentence_data.append({'Sentence': sentence, 'Ratio': ratio, 'Sentiment_score': sentiment_score})\n",
    "        \n",
    "    return pd.DataFrame(sentence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa9226-dec8-430c-9b44-420e8dbd5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def ratio_sen(text, parse, positive_categories, negative_categories):\n",
    "    text_sentence = sent_tokenize(text.lower())\n",
    "    sentence_data = []\n",
    "\n",
    "    for index, sentence in enumerate(text_sentence, start=1):\n",
    "        tokens = word_tokenize(sentence)\n",
    "        categories = [category for token in tokens for category in parse(token)]\n",
    "        positive_count = (sum(1 for category in categories if category in positive_categories))\n",
    "        negative_count = (sum(1 for category in categories if category in negative_categories))\n",
    "        \n",
    "        sentiment_intensity = abs(positive_count - negative_count)\n",
    "        \n",
    "        ratio = index / len(text_sentence)\n",
    "        sentence_data.append({'Sentence': sentence, 'Ratio': ratio, 'Sentiment_Intensity': sentiment_intensity})\n",
    "        \n",
    "    return pd.DataFrame(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6ff52-0f77-429e-b807-bbcc5649ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_func(df):\n",
    "    s = df['Ratio'].values\n",
    "    s2 = s ** 2\n",
    "    y = df['Sentiment_Intensity'].values\n",
    "\n",
    "    X = np.column_stack((s, s2))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    params = model.params\n",
    "    p_values = model.pvalues\n",
    "\n",
    "    return params, p_values\n",
    "\n",
    "\n",
    "def classify_emotional_genre(df):\n",
    "    s = df['Ratio'].values\n",
    "    sentiment_scores = df['Sentiment_Intensity'].values\n",
    "\n",
    "    if len(s) < 7:\n",
    "        return 'None'\n",
    "\n",
    "    params, pvalues = model_func(df)\n",
    "\n",
    "    if (params[1] > 0 and pvalues[1] < 0.05 and pvalues[2] > 0.05) or (\n",
    "            params[2] > 0 and pvalues[2] < 0.05): \n",
    "        genre = 'Progressive'\n",
    "    elif (params[1] < 0 and pvalues[1] < 0.05 and pvalues[2] > 0.05) or (\n",
    "            params[2] < 0 and pvalues[2] < 0.05): \n",
    "        genre = 'Regressive'\n",
    "    elif (params[1] < 0 and params[2] > 0 and pvalues[1] < 0.05 and pvalues[2] < 0.05): \n",
    "        genre = 'Comedy'\n",
    "    elif (params[1] > 0 and params[2] < 0 and pvalues[1] < 0.05 and pvalues[2] < 0.05):\n",
    "        genre = 'Tragedy'\n",
    "    elif pvalues[1] >= 0.05:\n",
    "        genre = 'Stable'\n",
    "    else:\n",
    "        genre = 'None'\n",
    "\n",
    "    return genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d4c70-aaf5-4109-8054-6a9167fb5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drama_feature(df):\n",
    "\n",
    "    if len(df) <= 2:\n",
    "        return 0  \n",
    "    \n",
    "    df['Emotional_change'] = df['Sentiment_score'].diff().abs()\n",
    "    \n",
    "    if len(df) > 1 and df['Emotional_change'][1:].notna().any():\n",
    "        max_change_index = df['Emotional_change'][1:].idxmax()\n",
    "        max_change_ratio = df.loc[max_change_index, 'Ratio']\n",
    "    else:\n",
    "        max_change_ratio = 0\n",
    "    \n",
    "    drama_ratio = 1 - max_change_ratio\n",
    "    \n",
    "    return drama_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf32fb2-fa7b-441b-aaac-6393f151ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_volatility(df):\n",
    "    if len(df) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        diffs = df['Sentiment_score'].diff().dropna().values\n",
    "        mean_diff = np.mean(diffs)\n",
    "        SV = np.sqrt(np.mean((diffs - mean_diff) ** 2))\n",
    "        return SV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f934215-53b7-4950-86d1-ab03edf94184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def narrative_feature(text):\n",
    "    liwc_path = 'LIWC2015 Dictionary.dic'\n",
    "    parse, category_names = liwc.load_token_parser(liwc_path)\n",
    "    \n",
    "    tembed = temporal_embedding(text, parse)\n",
    "    sembed = spacial_embedding(text, parse)\n",
    "\n",
    "    motion_categories = ['motion']\n",
    "    affective_process_categories = ['affect']\n",
    "    cognitive_insight_categories = ['insight']\n",
    "    positive_categories = ['posemo']\n",
    "    negative_categories = ['negemo']\n",
    "    loa_value, loc_value = calculate_features(text, parse, motion_categories, affective_process_categories, cognitive_insight_categories)\n",
    "    \n",
    "    sentiment_change = ratio_senti(text)\n",
    "    sentiment_genre = ratio_sen(text, parse, positive_categories, negative_categories)\n",
    "    text_genre = classify_emotional_genre(sentiment_genre)\n",
    "    drama = drama_feature(sentiment_change)\n",
    "    sv = sentiment_volatility(sentiment_change)\n",
    "    \n",
    "    genre_one_hot = {\n",
    "        \"Stable\": 0,\n",
    "        \"Progressive\": 0,\n",
    "        \"Regressive\": 0,\n",
    "        \"Comedy\": 0,\n",
    "        \"Tragedy\": 0\n",
    "    }\n",
    "    if text_genre in genre_one_hot:\n",
    "        genre_one_hot[text_genre] = 1\n",
    "\n",
    "    features = {\n",
    "        \"temporal_embedding\": tembed, \n",
    "        \"spacial_embedding\": sembed, \n",
    "        \"loa_value\": loa_value,\n",
    "        \"loc_value\": loc_value,\n",
    "        \"drama\": drama, \n",
    "        \"sentiment_volatility\": sv\n",
    "    }\n",
    "\n",
    "    features.update({f\"{k}\": v for k, v in genre_one_hot.items()})\n",
    "    \n",
    "    return pd.Series(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c9c00-b1f9-41db-ae75-29be42827990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52abce-f3ed-4737-99f6-3fe6e5d3df8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b88a58-4881-44df-8735-15b65604103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## behavior-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefcc1b-f52e-4fcf-904f-fc68e4b67096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_time_between_post(G):\n",
    "    times = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        time_str = data.get('time', '')\n",
    "        if time_str != '':\n",
    "            try:\n",
    "                time_int = int(time_str)\n",
    "                times.append(time_int)\n",
    "            except ValueError:\n",
    "                continue  \n",
    "\n",
    "    if len(times) < 2:\n",
    "        return 0  \n",
    "\n",
    "    times.sort()\n",
    "    time_diffs = [times[i+1] - times[i] for i in range(len(times) - 1)]\n",
    "\n",
    "    return sum(time_diffs) / len(time_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a6d3c-5af4-4bce-9def-3c51c2ffb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cascade_centrality(G):\n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    root_node = roots[0]\n",
    "    \n",
    "    repost_users = [attr['user_id'] for node, attr in G.nodes(data=True) if node != root_node]\n",
    "    repost_count = len(repost_users)\n",
    "    unique_user_count = len(set(repost_users))\n",
    "    \n",
    "    if repost_count == 0:\n",
    "        return 0\n",
    "\n",
    "    score = (1 - unique_user_count / repost_count)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300a173-8702-47d1-9aa1-1793402cfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def paticipant_num(G):\n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    root_node = roots[0]\n",
    "\n",
    "    user_ids = [attr['user_id'] for node, attr in G.nodes(data=True) if node != root_node]\n",
    "    user_counts = Counter(user_ids)\n",
    "\n",
    "    if len(user_counts) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    average_replies = sum(user_counts.values()) / len(user_counts) \n",
    "    max_replies_user = max(user_counts, key=user_counts.get)\n",
    "    max_replies_count = user_counts[max_replies_user]\n",
    "\n",
    "    return average_replies, max_replies_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73554ab9-0b3f-4fd8-9478-011e8b0e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_feature(G):\n",
    "    average_timediff_between_post = average_time_between_post(G)\n",
    "    cascade_centrality_score = cascade_centrality(G)\n",
    "    average_replies, max_replies = paticipant_num(G)\n",
    "\n",
    "    return { \n",
    "        \"average_time_between_post\": average_timediff_between_post,\n",
    "        \"cascade_centrality_score\": cascade_centrality_score,\n",
    "        \"avrage_replies\": average_replies,\n",
    "        \"max_replies\": max_replies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a808a3-c0fd-4e5d-bf71-224e3e524f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0813e-1605-4078-89b4-683ce56ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## propagation-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c967601-6fd0-429e-b123-538d0479ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_breadth(G, root):\n",
    "    depths = dict(nx.single_source_shortest_path_length(G, root))\n",
    "    max_depth = max(depths.values())\n",
    "    breadth = max(len([node for node, depth in depths.items() if depth == d]) for d in set(depths.values()))\n",
    "    return max_depth, breadth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531afc5e-bc00-49e6-86ea-a17d19e44fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def structural_virality(G):\n",
    "    shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "    path_lengths = np.zeros((len(G), len(G)))\n",
    "    for i, node_i in enumerate(G.nodes()):\n",
    "        for j, node_j in enumerate(G.nodes()):\n",
    "            if i != j and node_j in shortest_paths[node_i]:\n",
    "                path_lengths[i, j] = shortest_paths[node_i][node_j]\n",
    "\n",
    "    n = len(G.nodes())\n",
    "\n",
    "    if n > 1:\n",
    "        virality = (1 / (n * (n - 1))) * np.sum(path_lengths)\n",
    "    else:\n",
    "        virality = 0 \n",
    "        \n",
    "    return virality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe1489-3f45-4d14-9519-443966b227fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_sentiment(G):\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    non_root_nodes = [node for node in G.nodes if node not in roots]  \n",
    "\n",
    "    for node in non_root_nodes:\n",
    "        tweet_text = G.nodes[node].get('text', '')\n",
    "        if isinstance(tweet_text, list):\n",
    "            tweet_text = ' '.join(tweet_text)\n",
    "        if tweet_text: \n",
    "            sentiment = TextBlob(tweet_text)\n",
    "            sentiment_score = sentiment.sentiment.polarity\n",
    "            sentiment_scores.append(sentiment_score)\n",
    "\n",
    "    if sentiment_scores:\n",
    "        average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "    else:\n",
    "        average_sentiment = 0\n",
    "        \n",
    "    return average_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71f318-66d1-44c0-9417-abeb5a9f1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_enoji(G):\n",
    "    emoji_scores = []\n",
    "    \n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    non_root_nodes = [node for node in G.nodes if node not in roots] \n",
    "    \n",
    "    for node in non_root_nodes:\n",
    "        tweet_text = G.nodes[node].get('text', '')\n",
    "        if isinstance(tweet_text, list):\n",
    "            tweet_text = ' '.join(tweet_text)\n",
    "        if tweet_text: \n",
    "            emoji_score = emoji.emoji_count(tweet_text)\n",
    "            emoji_scores.append(emoji_score)\n",
    "            \n",
    "    if emoji_scores:\n",
    "        average_emoji = sum(emoji_scores) / len(emoji_scores)\n",
    "    else:\n",
    "        average_emoji = 0 \n",
    "\n",
    "    return average_emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee4c76-a3c4-4cc2-8260-1b2416006eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "file_path = r'NRC-Emotion-Intensity-Lexicon-v1.txt'\n",
    "\n",
    "column_names = ['words', 'emotion', 'intensity']\n",
    "emotion_intensity = pd.read_csv(file_path, delimiter='\\t', header=None, names=column_names)\n",
    "print(emotion_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb3a5f-b564-4ade-8118-72f44c8849cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "emotion_dict = {}   # {word: {sentiment: intensity}}\n",
    "for index, row in emotion_intensity.iterrows():\n",
    "    word = row['words']\n",
    "    emotion = row['emotion']\n",
    "    intensity = row['intensity']\n",
    "    if word not in emotion_dict:\n",
    "        emotion_dict[word] = {}\n",
    "    emotion_dict[word][emotion] = intensity\n",
    "\n",
    "def calculate_emotion_vector(text, emotion_dict):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    emotion_vector = defaultdict(float)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in emotion_dict:\n",
    "            for emotion, intensity in emotion_dict[word].items():\n",
    "                emotion_vector[emotion] += intensity\n",
    "\n",
    "    for emotion in emotions:\n",
    "        if emotion not in emotion_vector:\n",
    "            emotion_vector[emotion] = 0.0\n",
    "    \n",
    "    total_intensity = sum(emotion_vector.values())\n",
    "    if total_intensity > 0:\n",
    "        for emotion in emotion_vector:\n",
    "            emotion_vector[emotion] /= total_intensity\n",
    "    \n",
    "    return dict(emotion_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a44ffe-1dc1-4198-bac0-795c9d092894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_itensity(G):\n",
    "    \n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    non_root_nodes = [node for node in G.nodes if node not in roots]\n",
    "    \n",
    "    if not non_root_nodes:\n",
    "        return {emotion: 0 for emotion in emotions}\n",
    "    \n",
    "    accumulated_emotion_vector = defaultdict(float)\n",
    "    count = 0\n",
    "\n",
    "    for node in non_root_nodes:\n",
    "        tweet_text = G.nodes[node].get('text', '')\n",
    "        if isinstance(tweet_text, list):\n",
    "            tweet_text = ' '.join(tweet_text)\n",
    "        if tweet_text:\n",
    "            emotion_vector = calculate_emotion_vector(tweet_text, emotion_dict)\n",
    "            for emotion, intensity in emotion_vector.items():\n",
    "                accumulated_emotion_vector[emotion] += intensity\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        average_emotion_vector = {emotion: intensity / count for emotion, intensity in accumulated_emotion_vector.items()}\n",
    "    else:\n",
    "        average_emotion_vector = {emotion: 0 for emotion in emotions}\n",
    "    \n",
    "    return average_emotion_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60369f-3400-4e67-ae6a-3edbca2f2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation_feature(G):\n",
    "    retweet_reply_num = G.number_of_edges()\n",
    "    max_depth = 0\n",
    "    max_breath = 0 \n",
    "    roots = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    for root in roots:\n",
    "        depth, breath = depth_breadth(G, root)\n",
    "        if depth > max_depth:\n",
    "            max_depth = depth\n",
    "        if breath > max_breath:\n",
    "            max_breath = breath\n",
    "    unique_users = len(set(nx.get_node_attributes(G, 'user_id').values()))\n",
    "    structural_virality_score = structural_virality(G)\n",
    "    avg_sentiment = average_sentiment(G)\n",
    "    avg_emoji = average_enoji(G)\n",
    "    average_emotion_vector = average_itensity(G)\n",
    "\n",
    "    return {\n",
    "        \"retweet_reply_num\": retweet_reply_num, \n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_breath\": max_breath, \n",
    "        \"unique_users\": unique_users,\n",
    "        \"structural_virality_score\": structural_virality_score,\n",
    "        \"average_sentiment\": avg_sentiment,\n",
    "        \"avrtage_emoji\": avg_emoji,\n",
    "        \"anger_intensity\": average_emotion_vector['anger'],\n",
    "        \"anticipation_intensity\": average_emotion_vector['anticipation'],\n",
    "        \"disgust_intensity\": average_emotion_vector['disgust'],\n",
    "        \"fear_intensity\": average_emotion_vector['fear'],\n",
    "        \"joy_intensity\": average_emotion_vector['joy'],\n",
    "        \"sadness_intensity\": average_emotion_vector['sadness'],\n",
    "        \"surprise_intensity\": average_emotion_vector['surprise'],\n",
    "        \"trust_intensity\": average_emotion_vector['trust'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf8e93-8769-404a-b870-8b95cd3f2cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5ff8c-7630-47e4-abd3-b3bf5bb40156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3676229-f793-4217-bb7c-f5206f8a71ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a80ca-2381-46e8-be14-521e2f5fb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_content_feature(data):\n",
    "    feature_text = data['Text'].apply(textual_features)\n",
    "    feature_lexi = data['Text'].apply(lexicon_feature)\n",
    "    feature_narr = data['Text'].apply(narrative_feature)\n",
    "\n",
    "    text_data = pd.concat([feature_text, feature_lexi, feature_narr], axis=1)\n",
    "    \n",
    "    return text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b1374-e98a-496a-badd-5be7688129c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_beha_propa_feature(graph_data):\n",
    "    all_features = []\n",
    "    for graph_name, G in graph_data.items():\n",
    "\n",
    "        prop_features = propagation_feature(G)\n",
    "        beha_features = behavior_feature(G)\n",
    "\n",
    "        features = {\n",
    "            'id': graph_name,\n",
    "            **prop_features,\n",
    "            **beha_features\n",
    "        }\n",
    "\n",
    "        all_features.append(features)\n",
    "\n",
    "    graphdata = pd.DataFrame(all_features)\n",
    "\n",
    "    return graphdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636dbfed-7ae1-4810-9dc2-c05c98aab84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56614edc-788e-45c6-a8e7-d0f8574756b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## read data\n",
    "\n",
    "textdata = extract_content_feature(text_data)\n",
    "graphdata = extract_beha_propa_feature(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf88b57-8572-4726-8e5a-224ad1c9971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device=0)\n",
    "topics = [\"Politics\", \"Finance & Business\", \"Military\", \"Culture & Sports & Entertainment\", \"Society & Life\", \"Disasters & Accidents\", \"Education & Examinations\", \"Science & Technology\", \"Health & Medicine\"]\n",
    "\n",
    "text_data = pd.read_csv('')\n",
    "text_data['Topic'] = \"\"\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text = row['Text']\n",
    "    result = classifier(text, topics, multi_label=False)\n",
    "    text_data.at[index, 'Topic'] = result['labels'][0]\n",
    "    \n",
    "text_data = data.drop(columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569b9f7-b255-4db5-a6b5-272593885a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
